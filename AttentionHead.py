import tensorflow as tf

conv1d = tf.layers.conv1d


def attn_head(input_feat_seq, out_size, adj_mat, bias_mat,
              activation, include_weights=False, input_drop=0.0, coefficient_drop=0.0, residual=False):
    """ Builds a single attention head (layer) of the GAT architecture

        Parameters
        ----------
        input_feat_seq : tensor of shape (1, number_nodes, number_node_features:F)
            The tensor storing the node feature vectors for all the nodes of the input example graph
        out_size : int
            The dimensionality F' of the new node feat. vectors generated by this attention head
        adj_mat : tensor of shape (1, number_nodes, number_nodes)
            The tensor storing the adjacency matrix of the example graph with edge weights (not plain binary adjacency)
        bias_mat : tensor of shape (1, number_nodes, number_nodes)
            The tensor storing a bias matrix used for MASKED ATTENTION: discards the non-neighbours from the aggregation
        activation : function
            The activation function of the attention head applied to the feature vectors element-wise before outputting
        include_weights : bool
            Flag for integrating the EDGE WEIGHTS when generating the alpha_ij coefficients
        input_drop, coefficient_drop : float
            DROPOUT rate for the input features and the alpha_ij coefficients
        residual : bool
            Employ residual connections

        Returns
        -------
        ret : tensor of shape (1, number_nodes, out_size)
            The tensor storing the new feature vectors for all the nodes in the input graph
        """
    # name_scope is a context manager which adds the operations to the computation TensorFlow graph
    with tf.name_scope('my_attn'):
        # apply dropout to the input units of the layer
        if input_drop != 0.0:
            input_feat_seq = tf.nn.dropout(input_feat_seq, 1.0 - input_drop)

        # kernel size: the length of the 1D convolution window, in our case 1: kernel swiped only once on node_feats
        # this conv1D layer is the linear transformation using the learnable matrix W
        seq_fts = tf.layers.conv1d(input_feat_seq, filters=out_size, kernel_size=1, use_bias=False)
        # seq_fts is W*h_ifor all nodes i now

        # we apply the learnable weight vector a of length (2*F') to all pairs of  new feat.vec of length F'
        # applies the first half of a, outputs a real value (a partial dot product) for each feat.vec. in seq_fts
        f_1 = tf.layers.conv1d(seq_fts, 1, 1)
        # applies the other half of a as a partial dot product
        f_2 = tf.layers.conv1d(seq_fts, 1, 1)

        # logits - the matrix of the e_ij attention coefficients (e_ij and e_ji have different values)
        logits = f_1 + tf.transpose(f_2, [0, 2, 1])

        # compute the final coefficients alpha_ij  by applying ReLu, masked attention and normalize across neighbours
        alpha_coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)

        # integrate the edge weights w_ij of the graph to the e_ij coefficients
        if include_weights:
            alpha_coefs = tf.multiply(alpha_coefs, adj_mat)
            # alpha_coefs = tf.Print(alpha_coefs, [], message="alpha coeff: ", first_n=100, summarize=300)

        # apply DROPOUT to the resulted coefficients
        if coefficient_drop != 0.0:
            alpha_coefs = tf.nn.dropout(alpha_coefs, 1.0 - coefficient_drop)
        if input_drop != 0.0:
            seq_fts = tf.nn.dropout(seq_fts, 1.0 - input_drop)

        # compute h_i', the new feat. vec. for node i obtained from the attention aggregation of neighbour features
        vals = tf.matmul(alpha_coefs, seq_fts)
        # add a learnable bias
        ret = tf.contrib.layers.bias_add(vals)

        # residual connection
        if residual:
            if input_feat_seq.shape[-1] != ret.shape[-1]:
                ret = ret + conv1d(input_feat_seq, ret.shape[-1], 1)  # activation
            else:
                seq_fts = ret + input_feat_seq

        # calculate how many neighbours of each node contribute to the aggregation (non-zero alpha)
        non_zero_alpha = tf.count_nonzero(alpha_coefs, axis=-1)
        # non_zero_alpha = tf.Print(non_zero_alpha, [non_zero_alpha], message="This non_zero_alpha: ", summarize=64)
        # calculate the number of neighbours of each node
        degrees = tf.count_nonzero(adj_mat, axis=-1)
        # degrees = tf.Print(degrees, [degrees], message="This is degrees: ", summarize=64)

        # the UNIFORM LOSS of the current attention head
        loss_unif_attn = tf.reduce_mean(tf.to_float(tf.subtract(non_zero_alpha, degrees)))
        # loss_unif_attn = tf.Print(loss_unif_attn, [loss_unif_attn], message="This is loss_unif_attn for this a_head: ")

        # the EXCLUSIVE LOSS of the current attention head
        loss_excl_attn = tf.reduce_mean(tf.reduce_sum(tf.abs(alpha_coefs), axis=-1))
        # loss_excl_attn = tf.Print(loss_excl_attn, [loss_excl_attn], message="This is loss_excl_attn for this a_head: ")

        # apply activation of the attention head to the output features
        return activation(ret), loss_unif_attn, loss_excl_attn
